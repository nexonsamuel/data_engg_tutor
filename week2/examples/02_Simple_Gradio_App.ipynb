{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_cell",
   "metadata": {},
   "source": [
    "# Building a Simple Gradio Chat Interface\n",
    "\n",
    "This notebook demonstrates how to create interactive web-based interfaces using Gradio with a local language model (Llama 3.2) served via Ollama. We'll explore:\n",
    "- Basic model API calls\n",
    "- Streaming responses\n",
    "- Gradio interface with standard output\n",
    "- Gradio interface with real-time streaming markdown output\n",
    "\n",
    "Perfect for building simple chat applications that run locally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_heading",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "Import necessary libraries and establish connection to the local Ollama server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_heading",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "Load OpenAI client, display utilities, and Gradio framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572b1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "client_init_heading",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Client for Local Ollama\n",
    "\n",
    "Connect to the local Ollama server running on `localhost:11434`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88585e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama', \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_config_heading",
   "metadata": {},
   "source": [
    "### Define Model Configurations\n",
    "\n",
    "Specify which models to use from Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449cdc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model = 'mistral'\n",
    "llama_model = 'llama3.2:1b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_basic_response",
   "metadata": {},
   "source": [
    "## 2. Basic Model Response Functions\n",
    "\n",
    "Implement simple, non-streaming API calls to the Llama model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "system_msg_heading",
   "metadata": {},
   "source": [
    "### Define System Message & Basic Call Function\n",
    "\n",
    "Set up a helpful assistant persona and create a function for single-prompt API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6382f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = 'You are a helpful assistant'\n",
    "\n",
    "def call_llama(prompt):\n",
    "    print(\"Inside the call_llama function!!\")\n",
    "    messages = [\n",
    "        {'role': 'system',\n",
    "        'content': system_message},\n",
    "        {'role': 'user',\n",
    "        'content': prompt}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(model=llama_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_basic_heading",
   "metadata": {},
   "source": [
    "### Test Basic Model Call\n",
    "\n",
    "Demonstrate a simple question-answer interaction with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "342093c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside the call_llama function!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'July 4, 1776'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = call_llama(\"When did America get its independence, just give me the date?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_streaming",
   "metadata": {},
   "source": [
    "## 3. Streaming Response Functions\n",
    "\n",
    "Implement streaming API calls for real-time token-by-token output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stream_func_heading",
   "metadata": {},
   "source": [
    "### Streaming Model Call Function\n",
    "\n",
    "Create a generator function that yields model response tokens one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "584df51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_llama(prompt):\n",
    "    print(\"Inside the stream_llama function!!\")\n",
    "    messages = [\n",
    "        {'role': 'system',\n",
    "        'content': system_message},\n",
    "        {'role': 'user',\n",
    "        'content': prompt}\n",
    "    ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=llama_model, \n",
    "        messages=messages, \n",
    "        stream=True\n",
    "        )\n",
    "\n",
    "    result = \"\"\n",
    "    for text_chunk in stream:\n",
    "        content = text_chunk.choices[0].delta.content or \"\"\n",
    "        if content:\n",
    "            yield content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_stream_heading",
   "metadata": {},
   "source": [
    "### Test Streaming Response\n",
    "\n",
    "Demonstrate the streaming output in the notebook console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d4ea009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside the stream_llama function!!\n",
      "July 4, 1776.\n"
     ]
    }
   ],
   "source": [
    "result = \"\"\n",
    "for text in stream_llama(\"When did America get its independence, just give me the date?\"):\n",
    "    print(text, end=\"\", flush=True)\n",
    "print()  # New line at end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_gradio_basic",
   "metadata": {},
   "source": [
    "## 4. Gradio Interface - Basic Text Output\n",
    "\n",
    "Create a web-based chat interface using Gradio with accumulated response in a textbox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio_basic_heading",
   "metadata": {},
   "source": [
    "### Interactive Chat App with Textbox Output\n",
    "\n",
    "Build a Gradio interface that displays the model's complete response in a textbox. Includes example prompts for quick testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4cc40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside the call_llama function!!\n"
     ]
    }
   ],
   "source": [
    "input_textbox = gr.Textbox(label='Your Message', info='Enter a message', lines=7)\n",
    "output_textbox = gr.Textbox(label='Model response', lines=8)\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=call_llama,\n",
    "    title='My First Gradio App',\n",
    "    inputs=input_textbox,\n",
    "    outputs=output_textbox, \n",
    "    examples=[\n",
    "        \"If gravity were even slightly stronger or weaker, the universe would collapse into chaos or drift into lifeless emptiness, and such delicate precision whispers not of accident, but of intentional design.\",\n",
    "        \"Should society prohibit euthanasia to protect life, even if doing so prolongs suffering?\",\n",
    "        \"You have to be a fighter, because if you don't fight for your love, what kind of love do you have\",\n",
    "        \"Ever tried, ever failed, no matter, try again, fail again, fail better\",\n",
    "        \"At the stroke of the midnight hour, when the world sleeps, India will awake to life and freedom\",\n",
    "        \"I have a dream that one day this nation will rise up and live out the true meaning of its creed.\"\n",
    "    ],\n",
    "    flagging_mode='never'\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_gradio_streaming",
   "metadata": {},
   "source": [
    "## 5. Gradio Interface - Streaming Markdown Output\n",
    "\n",
    "Enhance the interface with real-time streaming response displayed in markdown format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio_stream_wrapper_heading",
   "metadata": {},
   "source": [
    "### Create Streaming Wrapper Function\n",
    "\n",
    "Accumulate streaming tokens and yield progressive response updates for Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a9aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_llama_stream(prompt):\n",
    "    result = \"\"\n",
    "    for token in stream_llama(prompt):\n",
    "        result += token\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio_stream_app_heading",
   "metadata": {},
   "source": [
    "### Interactive Chat App with Streaming Markdown Output\n",
    "\n",
    "Build a Gradio interface that displays the model's response in real-time as markdown. The response updates progressively as tokens are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6bf783b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside the stream_llama function!!\n"
     ]
    }
   ],
   "source": [
    "input_textbox = gr.Textbox(label='Your Message', info='Enter a message', lines=7)\n",
    "output_textbox = gr.Markdown(label='Model response')\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=gradio_llama_stream,\n",
    "    title='My First Gradio App',\n",
    "    inputs=input_textbox,\n",
    "    outputs=output_textbox, \n",
    "    examples=[\n",
    "        \"If gravity were even slightly stronger or weaker, the universe would collapse into chaos or drift into lifeless emptiness, and such delicate precision whispers not of accident, but of intentional design.\",\n",
    "        \"Should society prohibit euthanasia to protect life, even if doing so prolongs suffering?\",\n",
    "        \"You have to be a fighter, because if you don't fight for your love, what kind of love do you have\",\n",
    "        \"Ever tried, ever failed, no matter, try again, fail again, fail better\",\n",
    "        \"At the stroke of the midnight hour, when the world sleeps, India will awake to life and freedom\",\n",
    "        \"I have a dream that one day this nation will rise up and live out the true meaning of its creed.\"\n",
    "    ],\n",
    "    flagging_mode='never'\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing_cell",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Thank You!\n",
    "\n",
    "You've now built interactive chat interfaces using Gradio! Explore further by:\n",
    "- Modifying the system message for different personas\n",
    "- Adding more example prompts\n",
    "- Switching between Mistral and Llama models\n",
    "- Extending the interface with additional features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
