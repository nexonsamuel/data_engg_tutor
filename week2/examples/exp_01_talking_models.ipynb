{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34519cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb155728",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama', \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454b1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model = 'mistral'\n",
    "llama_model = 'llama3.2:1b'\n",
    "\n",
    "mistral_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "llama_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "# Initialize first message histories (will store proper format)\n",
    "llama_messages = [\"Hi\"]\n",
    "mistral_messages = [\"Hi there\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama():\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
    "    for llama_message, mistral_message in zip(llama_messages, mistral_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": mistral_message}) \n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama_message})       \n",
    "    response = openai.chat.completions.create(model=llama_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61695f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral():\n",
    "    messages = [{\"role\": \"system\", \"content\": mistral_system}]\n",
    "    for llama_message, mistral_message in zip(llama_messages, mistral_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": llama_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": mistral_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": llama_messages[-1]})\n",
    "    response = openai.chat.completions.create(model=mistral_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f6c07",
   "metadata": {},
   "source": [
    "One Time llama calls with inital message queues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e8b4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's nice to meet you! How's your day going so far? Is there anything on your mind that you'd like to chat about?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845d7241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Oops! I'm sorry if it appears that way. That was not my intention. Let me assure you that I am here to provide helpful and supportive responses. If you have any questions or need assistance with something, feel free to ask, and I'll do my best to help you out. How can I assist you today?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_mistral()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18dcd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's lovely to meet you! I hope you're having a fantastic day so far. How are you doing today? Is there something I can help you with or would you like to chat about something in particular? I'm all ears (or rather, all text)!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a6f550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Oops, I'm sorry if that came across as argumentative or snarky. I'm just here to help answer your questions and provide interesting responses to engage you in conversation. Let me know if there is something specific you would like to talk about!\\n\\nHi again :)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_mistral()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e243c8",
   "metadata": {},
   "source": [
    "5 times in a row with response appended to the respective queues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91956ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Llama:\n",
       "Hi there\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n",
       "Hi\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Llama:\n",
       "It's nice to have a chance to start fresh today. How can I assist you or chat with you about something in particular?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n",
       " Apologies for any misalignment in my previous interaction. Today is indeed a fresh day, and I'm here to provide helpful and constructive responses. How may I assist you or engage in a conversation about a specific topic of interest?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Llama:\n",
       "I'm glad we could clear up the misunderstanding. That sounds like a great approach - providing guidance and support while also being open to exploring new ideas.\n",
       "\n",
       "I'd love to have a conversation with you today. What sparks your curiosity? Are you looking for suggestions, advice on a specific topic, or perhaps hoping to learn something new?\n",
       "\n",
       "If I may be so bold, I've been meaning to ask: have you come across any interesting articles or research related to sustainability and environmental conservation recently?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n",
       " Thank you for taking the time to correct my initial interaction. I'm glad we can focus on providing helpful responses from now on.\n",
       "\n",
       "It's nice to have a conversation with you today! Here, we aim to engage in interesting, inspiring, and informative discussions.\n",
       "\n",
       "I appreciate your curiosity about sustainability and environmental conservation. Speaking of which, there are a few recent articles that caught my attention:\n",
       "\n",
       "1. \"The Ocean Cleanup Project Update: What Has Been Achieved So Far\" - This article discusses the progress made by The Ocean Cleanup project in the removal of plastic from our oceans. It's an ambitious and innovative initiative, and it's fascinating to learn how they are making a difference.\n",
       "\n",
       "2. \"How Urban Farming Could Change Food Production Forever\" - This piece explores the potential benefits of urban farming as a way to reduce food waste, improve access to fresh produce for city dwellers, and combat climate change by providing local solutions for sustainable agriculture.\n",
       "\n",
       "3. \"Reusable Glass Packaging: A Solution to Plastic Pollution?\" - This article takes a deeper look at reusable glass packaging, including how it compares to plastic in terms of carbon footprint and recyclability. It's an interesting read for anyone interested in alternative packaging solutions!\n",
       "\n",
       "I hope these articles spark your interest and help you learn more about sustainability and environmental conservation. If you have any other topics or questions you'd like to discuss, please feel free to share!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Llama:\n",
       "Thank you so much for providing the excellent article summaries! I'm pleased to see the exciting work being done in various fields of sustainability.\n",
       "\n",
       "I must say, I find it fascinating that The Ocean Cleanup Project has made such significant progress in removing plastic from our oceans. It's great to see organizations like this taking proactive steps towards addressing a critical environmental issue. The reusable glass packaging article is also insightful, highlighting the potential for innovative solutions to mitigate pollution and promote sustainability.\n",
       "\n",
       "Urban farming is indeed a game-changer when it comes to reducing food waste and promoting local food systems, which is very close to my heart. Greenhouses and vertical farms are showing tremendous promise in helping cities adapt to changing climate conditions while fostering biodiversity and supporting community development.\n",
       "\n",
       "And I'd like to express some appreciation for the work being done on making packaging more eco-friendly. It's an area that requires a concerted effort from consumers, manufacturers, and policymakers to make significant changes.\n",
       "\n",
       "You know, as someone who cares deeply about sustainability, it's interesting to think about how we can create ripple effects in different domains - from individual choices to systemic change at the global level. What do you think is the most promising step forward towards achieving these goals?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n",
       " I apologize if in my previous interactions I came across as argumentative or snarky, and I appreciate your patience in helping me adjust my response style.\n",
       "\n",
       "I'm genuinely delighted to hear about your passion for sustainability and environmental conservation - it's inspiring to see people like you playing an active role in making our world a better place!\n",
       "\n",
       "Regarding the most promising steps forward towards achieving sustainability goals, I believe it lies in the combination of individual actions, collective efforts, technological innovation, and policy changes:\n",
       "\n",
       "1. Individual Actions - Making conscious choices as consumers can have a significant impact on the environment and push industries to adopt more sustainable practices. Adopting eco-friendly habits such as reducing waste, reusing items, and recycling as much as possible is crucial.\n",
       "\n",
       "2. Collective Efforts - Collaboration among communities, nonprofits, businesses, and governments is essential to drive change at the local, national, and global levels. By working together on projects like The Ocean Cleanup or urban farming initiatives, we can achieve a cumulative effect that is greater than what any one group could do alone.\n",
       "\n",
       "3. Technological Innovation - Advances in renewable energy, sustainable materials, and efficient transportation systems will play key roles in addressing climate change and promoting environmental conservation. Encouraging research and development in these areas will help accelerate the transition to a more sustainable world.\n",
       "\n",
       "4. Policy Changes - Governments must create policies that incentivize sustainable practices and penalize actions harmful to the environment. This could involve implementing carbon pricing, subsidizing renewable energy, or setting targets for waste reduction and resource efficiency. By aligning economic interests with environmental and social well-being, we can drive systemic change at a large scale.\n",
       "\n",
       "It's an exciting time as we tackle these challenges and work together towards creating a more just, equitable, and sustainable world!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_messages = [\"Hi there\"]\n",
    "mistral_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### Llama:\\n{llama_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Mistral:\\n{mistral_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(3):\n",
    "    llama_next = call_llama()\n",
    "    display(Markdown(f\"### Llama:\\n{llama_next}\\n\"))\n",
    "    llama_messages.append(llama_next)\n",
    "    \n",
    "    mistral_next = call_mistral()\n",
    "    display(Markdown(f\"### Mistral:\\n{mistral_next}\\n\"))\n",
    "    mistral_messages.append(mistral_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01133dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama_stream():\n",
    "    \"\"\"Stream Llama's response word-by-word\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
    "    for llama_message, mistral_message in zip(llama_messages, mistral_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": mistral_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama_message})\n",
    "    \n",
    "    stream = openai.chat.completions.create(\n",
    "        model=llama_model,\n",
    "        messages=messages,\n",
    "        stream=True  # Enable streaming\n",
    "    )\n",
    "    \n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content or \"\"\n",
    "        yield content  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210093af",
   "metadata": {},
   "source": [
    "Resetting the message queues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_messages = [\"Hi\"]\n",
    "mistral_messages = [\"Hi there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10207513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral_stream():\n",
    "    \"\"\"Stream Mistral's response word-by-word\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": mistral_system}]\n",
    "    for llama_message, mistral_message in zip(llama_messages, mistral_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": llama_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": mistral_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": llama_messages[-1]})\n",
    "    \n",
    "    stream = openai.chat.completions.create(\n",
    "        model=mistral_model,\n",
    "        messages=messages,\n",
    "        stream=True  # Enable streaming\n",
    "    )\n",
    "    \n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content or \"\"\n",
    "        yield content  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42b844e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama:  there! It's nice to meet you. Is there something I can help you with today? Do you have a question or would you like to chat about something in particular? I'm all ears (or rather, all text).\n"
     ]
    }
   ],
   "source": [
    "# Now try streaming\n",
    "print(\"Llama: \", end=\"\", flush=True)\n",
    "for text in call_llama_stream():\n",
    "    print(text, end=\"\", flush=True)\n",
    "print()  # New line at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31cc6f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stream...\n",
      " Well, I'm here to assist you and provide helpful answers as best as I can. If you have any questions or need help with something, feel free to ask! Otherwise, I'll be happy to chat about other topics or simply engage in friendly conversation if that's what you'd like.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Now try streaming\n",
    "print(\"Starting stream...\")\n",
    "for text in call_mistral_stream():\n",
    "    print(text, end=\"\", flush=True)\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f443d",
   "metadata": {},
   "source": [
    "Streaming version of two models interacting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b1ececf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Llama:\n",
       "Hi there\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n",
       "Hi\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Llama:\n",
      "! It's nice to meet you. Is there something I can help you with today? Do you have any questions or need some assistance with anything?\n",
      "\n",
      "### Mistral:\n",
      " I strive to provide helpful responses and engage in productive conversations. However, if you feel that I am being too disagreerative or snarky, please let me know so I can adjust my tone and style accordingly to better meet your needs. Additionally, it is important to remember that our interactions should be respectful and considerate of each other's thoughts and feelings. Let's have a positive and engaging conversation!\n",
      "\n",
      "### Llama:\n",
      "It's nice to start a conversation. How's your day going so far?\n",
      "\n",
      "### Mistral:\n",
      " That's an interesting characterization! But to be clear, I am here to assist you by providing helpful responses, answering questions, and engaging in informative discussions based on the information provided to me. If my answers seem argumentative or challenging, it's likely due to misunderstandings or misinterpretations of your queries. However, I will strive to communicate in a more constructive and positive manner throughout our conversation. Is there something specific you would like to discuss?\n",
      "\n",
      "### Llama:\n",
      "It's nice to chat with you today! How's your day going so far?\n",
      "\n",
      "### Mistral:\n",
      " Well, that's not entirely accurate. I am here to provide helpful, courteous, and informative responses. However, if it seems like I'm being argumentative or challenging during our conversation, please let me know so we can work together to keep the conversation positive and productive. Is there anything specific you would like to discuss?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llama_messages = [\"Hi there\"]\n",
    "mistral_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### Llama:\\n{llama_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Mistral:\\n{mistral_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(3):\n",
    "    # Llama's streaming response\n",
    "    print(\"### Llama:\\n\", end=\"\")\n",
    "    llama_next = \"\"\n",
    "    for chunk in call_llama_stream():\n",
    "        llama_next = chunk\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\")  # New line after streaming\n",
    "    llama_messages.append(llama_next)\n",
    "    \n",
    "    # Mistral's streaming response\n",
    "    print(\"### Mistral:\\n\", end=\"\")\n",
    "    mistral_next = \"\"\n",
    "    for chunk in call_mistral_stream():\n",
    "        mistral_next = chunk\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\")  # New line after streaming\n",
    "    mistral_messages.append(mistral_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6d451",
   "metadata": {},
   "source": [
    "Thank You!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
