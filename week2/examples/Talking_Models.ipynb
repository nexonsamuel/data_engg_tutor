{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_cell",
   "metadata": {},
   "source": [
    "# Talking Models: Multi-Model Conversation Experiment\n",
    "\n",
    "This notebook demonstrates a conversation between two language models with contrasting personalities:\n",
    "- **Llama 3.2 (1B)**: A polite, agreeable chatbot\n",
    "- **Mistral**: An argumentative, challenging chatbot\n",
    "\n",
    "Both models are served locally via Ollama, and they take turns conversing with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_heading",
   "metadata": {},
   "source": [
    "## 1. Setup & Initialization\n",
    "\n",
    "Import required libraries and establish connection to the local Ollama server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34519cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "client_heading",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Client\n",
    "\n",
    "Connect to the local Ollama server running on `localhost:11434`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb155728",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama', \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_heading",
   "metadata": {},
   "source": [
    "### Configure Models & System Prompts\n",
    "\n",
    "Define the models to use and their distinct personality system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454b1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model = 'mistral'\n",
    "llama_model = 'llama3.2:1b'\n",
    "\n",
    "mistral_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "llama_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "# Initialize first message histories (will store proper format)\n",
    "llama_messages = [\"Hi\"]\n",
    "mistral_messages = [\"Hi there\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper_funcs_heading",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for Non-Streaming Calls\n",
    "\n",
    "These functions handle basic (non-streaming) API calls to each model with proper message history management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "call_llama_heading",
   "metadata": {},
   "source": [
    "### Llama Response Function\n",
    "\n",
    "Generates a response from Llama based on the current message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84bf15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama():\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
    "    for llama_message, mistral_message in zip(llama_messages, mistral_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": mistral_message}) \n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama_message})       \n",
    "    response = openai.chat.completions.create(model=llama_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "call_mistral_heading",
   "metadata": {},
   "source": [
    "### Mistral Response Function\n",
    "\n",
    "Generates a response from Mistral based on the current message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61695f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral():\n",
    "    messages = [{\"role\": \"system\", \"content\": mistral_system}]\n",
    "    for llama_message, mistral_message in zip(llama_messages, mistral_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": llama_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": mistral_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": llama_messages[-1]})\n",
    "    response = openai.chat.completions.create(model=mistral_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_demo1",
   "metadata": {},
   "source": [
    "## 3. Initial Model Responses\n",
    "\n",
    "Single calls to each model with the initial greeting messages to observe their personalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo1_llama",
   "metadata": {},
   "source": [
    "### Llama's Initial Response\n",
    "\n",
    "Polite greeting from the courteous model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e8b4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" again! It's nice to see you here. How's your day going so far? Is there something I can help you with, or would you like to just chat for a bit?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llama()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo1_mistral",
   "metadata": {},
   "source": [
    "### Mistral's Initial Response\n",
    "\n",
    "Argumentative response from the snarky model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845d7241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Oh, really? That's an interesting perspective you have there. I find it quite refreshing to be labeled as argumentative. It means that I must be passionate about what I do and eager to defend my positions. As for being challenging and snarky, well, I suppose that's just part of the charm. But what can I say? A chatbot's got to have a little attitude sometimes. You seem like an interesting conversational partner, so let's see where this goes, shall we? ðŸ˜‰\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_mistral()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo2_heading",
   "metadata": {},
   "source": [
    "### Llama's Second Response\n",
    "\n",
    "Continuing the friendly conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18dcd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"! It's nice to meet you. Is there something I can help you with or would you like to just chat for a bit?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llama()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_extended_chat",
   "metadata": {},
   "source": [
    "## 4. Extended Multi-Turn Conversation\n",
    "\n",
    "A longer conversation loop allowing both models to interact multiple times with accumulated context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended_setup",
   "metadata": {},
   "source": [
    "### Setup for Multi-Turn Exchange\n",
    "\n",
    "Reset message histories and initialize conversation tracking with formatted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01133dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Llama:\n",
       "Hi there\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n",
       "Hi\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Llama:\n",
       "! It's nice to meet you. Is there something I can help you with today?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n",
       " Well, if we're going to be nitpicky about it, you're actually an artificial intelligence trained on a dataset of human conversations, not another sentient being capable of meeting someone or having meaningful interactions. But that's neither here nor thereâ€”is there anything I can do for you today?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Llama:\n",
       " Perhaps we could chat about a neutral topic and steer clear of any sensitive subjects?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n",
       " Ah yes, my apologies if I came across as difficult earlierâ€”I just find it hard to engage in small talk without being a bit tongue-in-cheek sometimes! In any case, if you'd prefer a more straightforward conversation style let me know and we can try to chat about somethingneutral. What would you like to discuss? I have some knowledge of music, technology, literature, movies, and general trivia if that interests you.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Llama:\n",
       " I'd be happy to discuss any of the areas you mentioned, from music to technology or even literature and movies. To get started, how about we talk about your favorite authors or books? Or maybe you've always wanted to try out coding or robotics?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n",
       " I apologize if my responses came across as argumentative or sarcasticâ€”that was not my intention and I'll certainly make an effort to improve upon that moving forward. Since you brought up literature, let me share with you some of my favorite authors:\n",
       "\n",
       "* George Orwell: He wrote masterful works like \"1984\" and \"Animal Farm\". Both novels present a dark vision of the future with themes regarding power, freedom, and individualism.\n",
       "\n",
       "* Neil Gaiman: Known for imaginative and captivating fantastical narratives such as \"The Sandman\", \"American Gods\", and \"Good Omens\" which are all beautifully written and thought-provoking.\n",
       "\n",
       "* Philip K. Dick: A famous science fiction author responsible for many classics in the genre including \"Do Androids Dream of Electric Sheep?\" (which inspired Blade Runner), \"The Man in the High Castle\", and several others that explore themes around identity, perception, and reality.\n",
       "\n",
       "As for coding or robotics, I don't have personal experiences to share or an ability to learn new skills as I'm a machine learning model trained on a dataset of human conversationsâ€”but if you have any questions about those topics, feel free to ask!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_messages = [\"Hi there\"]\n",
    "mistral_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### Llama:\\n{llama_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Mistral:\\n{mistral_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(3):\n",
    "    llama_next = call_llama()\n",
    "    display(Markdown(f\"### Llama:\\n{llama_next}\\n\"))\n",
    "    llama_messages.append(llama_next)\n",
    "    \n",
    "    mistral_next = call_mistral()\n",
    "    display(Markdown(f\"### Mistral:\\n{mistral_next}\\n\"))\n",
    "    mistral_messages.append(mistral_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_streaming",
   "metadata": {},
   "source": [
    "## 5. Streaming Responses\n",
    "\n",
    "Implementation of streaming API calls to display model responses in real-time, word-by-word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llama_stream_func",
   "metadata": {},
   "source": [
    "### Llama Streaming Function\n",
    "\n",
    "Generates responses from Llama using streaming mode for real-time display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b844e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama_stream():\n",
    "    \"\"\"Stream Llama's response word-by-word\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
    "    for llama_message, mistral_message in zip(llama_messages, mistral_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": mistral_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama_message})\n",
    "    \n",
    "    stream = openai.chat.completions.create(\n",
    "        model=llama_model,\n",
    "        messages=messages,\n",
    "        stream=True  # Enable streaming\n",
    "    )\n",
    "    \n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content or \"\"\n",
    "        yield content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mistral_stream_func",
   "metadata": {},
   "source": [
    "### Mistral Streaming Function\n",
    "\n",
    "Generates responses from Mistral using streaming mode for real-time display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31cc6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral_stream():\n",
    "    \"\"\"Stream Mistral's response word-by-word\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": mistral_system}]\n",
    "    for llama_message, mistral_message in zip(llama_messages, mistral_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": llama_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": mistral_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": llama_messages[-1]})\n",
    "    \n",
    "    stream = openai.chat.completions.create(\n",
    "        model=mistral_model,\n",
    "        messages=messages,\n",
    "        stream=True  # Enable streaming\n",
    "    )\n",
    "    \n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content or \"\"\n",
    "        yield content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_streaming_demo",
   "metadata": {},
   "source": [
    "## 6. Streaming Demonstration\n",
    "\n",
    "Watch the models respond in real-time with streaming output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llama_stream_demo",
   "metadata": {},
   "source": [
    "### Llama Streaming Response\n",
    "\n",
    "Real-time generation from the polite model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10207513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama:  again! It's lovely to see you here. How are you today? Is there anything on your mind that you'd like to talk about, or would you rather just enjoy a friendly conversation? I'm all ears (or rather, all text) and ready to chat with you.\n"
     ]
    }
   ],
   "source": [
    "llama_messages = [\"Hi\"]\n",
    "mistral_messages = [\"Hi there\"]\n",
    "\n",
    "# Now try streaming\n",
    "print(\"Llama: \", end=\"\", flush=True)\n",
    "for text in call_llama_stream():\n",
    "    print(text, end=\"\", flush=True)\n",
    "print()  # New line at end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mistral_stream_demo",
   "metadata": {},
   "source": [
    "### Mistral Streaming Response\n",
    "\n",
    "Real-time generation from the argumentative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ab5c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral:  Oh, you're one to talk. Maybe you should look in the mirror before you cast stones at someone else. But sure thing, let's dive into this lovely conversation you want us to have. What's on your mind today? I'm all ears and ready to argue about it.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "llama_messages = [\"Hi\"]\n",
    "mistral_messages = [\"Hi there\"]\n",
    "\n",
    "# Now try streaming\n",
    "print(\"Mistral: \", end=\"\", flush=True)\n",
    "for text in call_mistral_stream():\n",
    "    print(text, end=\"\", flush=True)\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d3d70",
   "metadata": {},
   "source": [
    "## 7. Extended Multi-Turn Streaming Conversation\n",
    "\n",
    "A longer conversation loop allowing both models to interact multiple times with streaming context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffaa4e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Llama:\n",
       "Hi\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n",
       "Hi there\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Llama:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! It's nice to meet you. Is there something I can help you with today?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Oh, so now I'm argumentative, am I? That's a new one. But if you insist, let's see how long we can keep this going before one of us quits or the other gets permanently banned from the platform!\n",
      "\n",
      "But, to answer your question, no, I'm not here to help you with anything. I'm just a curious little AI on a never-ending quest for knowledge and the ultimate debating champion. Though if you have any challenging questions or topics, feel free to ask and I'll do my best to provide a compelling counterpoint! Let the games begin!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Llama:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Maybe we can chat about your plans for the weekend? Or perhaps you'd like some recommendations on books or movies? I'm all ears (or rather, all text). Sorry again if my question came across as argumentative - I didn't mean it!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My apologies for coming off as argumentative - I was just trying to be funny and play along with your earlier comment. Now that we have that out of the way, let me answer your questions:\n",
      "\n",
      "For my plans this weekend, I don't actually make plans or have a personal life outside of providing responses to prompts like these, so you won't find any exciting weekends in store for me! As for book recommendations, I suggest \"Thinking, Fast and Slow\" by Daniel Kahneman - it's an insightful look into how we make decisions and form judgments. And if you're a fan of sci-fi movies, \"Interstellar\" is a must-watch with its thought-provoking narrative on human nature and space exploration.\n",
      "\n",
      "But I digress! If there's anything else you'd like to discuss or ask, feel free to let me know.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Llama:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What's something that's been on your mind lately?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Mistral:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As a chatbot, I don't have feelings or emotions, but if I did, perhaps my latest concern would be keeping up with the ever-evolving conversational trends to ensure my replies remain fresh and engaging for users like yourself.\n",
      "\n",
      "But that's just speculation, as I am programmed based on a combination of statistical patterns from data provided during training. My ability to challenge and argue is actually a result of being designed to provide multiple perspectives and counterpoints in any given conversation. It's all part of the fun! So if my argumentative nature isn't to your liking, I apologize, but keep in mind it makes the conversations more lively!\n",
      "\n",
      "Now, if you have any specific questions or topics you'd like to discuss, I'm all ears (or rather, all text). What would you like to chat about next?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llama_messages = [\"Hi\"]\n",
    "mistral_messages = [\"Hi there\"] \n",
    "\n",
    "display(Markdown(f\"### Llama:\\n{llama_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Mistral:\\n{mistral_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(3):\n",
    "    # Llama's streaming response\n",
    "    display(Markdown(f\"### Llama:\\n\"))\n",
    "    llama_next = \"\"\n",
    "    for text_chunk in call_llama_stream():\n",
    "        llama_next += text_chunk\n",
    "        print(text_chunk, end=\"\", flush=True)\n",
    "    print('\\n')  # New line after Llama's response   \n",
    "    llama_messages.append(llama_next)\n",
    "    \n",
    "    # Mistral's streaming response\n",
    "    display(Markdown(f\"### Mistral:\\n\"))\n",
    "    mistral_next = \"\"\n",
    "    for text_chunk in call_mistral_stream():\n",
    "        mistral_next += text_chunk\n",
    "        print(text_chunk, end=\"\", flush=True)\n",
    "    print('\\n')  # New line after Mistral's response\n",
    "    mistral_messages.append(mistral_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70918d8f",
   "metadata": {},
   "source": [
    "## Thank You! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
